---
title: "homework"
author: "KaiLi Wang"
date: "2023-12-08"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## HW 0


### example 1

画出$sin（x）$的折线图，颜色为red，添加$cos(x)$的折线图，颜色为blue，并添加对应曲线的公式和图例


```{r cars}
plot(seq(0,2*pi,length=200),sin(seq(0,2*pi,length=200)),type='l',col='red',xlab='x',ylab='y')
lines(seq(0,2*pi,length=200),cos(seq(0,2*pi,length=200)),col='blue')
text(2.4,0.7,"sin(x)")
text(5.5,0.7,"cos(x)")
legend(0,-0.5,col=c("red","blue"),lty=c(1,1),lwd=c(1,1),legend=c("sin","cos"))
```

### example 2

将1:9按列生成一个3 ×3的矩阵A，将1:3作为作为对角元素生成3 ×3的对角矩阵B。再将矩阵B的逆按行合并矩阵A
 

```{r}
A<-matrix(1:9,nrow=3,ncol=3)
B<-diag(1:3)
rbind(solve(B),A)
```

矩阵B的逆按行合并矩阵A得到矩阵：

\begin{pmatrix}
1&0&0 \\
0&0.5&0 \\
0&0&0.333 \\
1&4&7 \\
2&5&8 \\
3&6&9 \\
\end{pmatrix}



计算A和B矩阵的内积$(A^TB
)C$，并寻找矩阵C中的最小元素，并返回其位置的行号和列号

```{r}
C<-crossprod(A,B)
C
C[which(C==min(C,na.rm=T))]
which(C==min(C,na.rm=T),arr.ind=T)
```

最小元素|位置行号|位置列号
:-:|:-:|:-:
1|1|1


### example 3

$X\in U(0,1),Y\in U(0,1)$，各生成100个随机数构建一个数据框，画出xy的简单散点图

```{r}
library(ggplot2)

set.seed(1)
data <- data.frame(
  x = rnorm(100),
  y = rnorm(100)
)

plot <- ggplot(data, aes(x = x, y = y)) +
  geom_point() +                      
  labs(title = "简单散点图",            
       x = "X 轴",                    
       y = "Y 轴")        

print(plot)
```

## HW 1

### 3.4

The Rayleigh density is

$$
f(x)=\frac{x}{\sigma^2} e^{-x^2 /\left(2 \sigma^2\right)}, \quad x \geq 0, \sigma>0
$$
Develop an algorithm to generate random samples from a Rayleigh $(\sigma)$ distribution. Generate Rayleigh $(\sigma)$ samples for several choices of $\sigma>0$ and check that the mode of the generated samples is close to the theoretical mode $\sigma$ (check the histogram).


```{r}
rayleigh<-function(n,sigma){
  u<-runif(n)
  x<-sqrt(-2*sigma^2*log(1-u))
  return(x)
}
ray<-rayleigh(10000,1)
hist(ray,prob="T",breaks=20,main="Histogram of Rayleigh(1)")
y <- seq(0, 4, 0.01)
lines(y, y*exp(-y^2/2))
text(2.5,0.5,expression(f(x)==x*e^{-x^2/2}))
hist(rayleigh(10000,2),prob="T",breaks=20,main="Histogram of Rayleigh(2)")
y <- seq(0, 8, 0.01)
lines(y, y*exp(-y^2/8)/4)
text(5,0.25,expression(f(x)=={(x/4)}*e^{-x^2/8}))
```

从图中可以看出，第一幅图中曲线的最高点和直方图的最高点都在1附近与Rayleigh(1)分布理论众数一致；第二幅图中曲线的最高点和直方图的最高点都在2附近与Rayleigh(2)分布理论众数一致


### 3.11

Generate a random sample of size 1000 from a normal location mixture. The components of the mixture have $N(0,1)$ and $N(3,1)$ distributions with mixing probabilities $p_1$ and $p_2=1-p_1$. Graph the histogram of the sample with density superimposed, for $p_1=0.75$. Repeat with different values for $p_1$ and observe whether the empirical distribution of the mixture appears to be bimodal. Make a conjecture about the values of $p_1$ that produce bimodal mixtures.



```{r}
normal_mixture<-function(n,p){
  x1<-rnorm(n,0,1)
  x2<-rnorm(n,3,1)
  z<-rep(0,n)
  u<-runif(n)
  for(i in 1:n){
    if(u[i]<p){
      z[i]<-x1[i]
    }else{
      z[i]<-x2[i]
    }
  }
  return(z)
}
x<-normal_mixture(1000,0.75)
hist(x,breaks = 20,main="Histogram of normal mixture of p1=0.75")
hist(normal_mixture(1000,0.5),breaks = 30,main="Histogram of normal mixture of p1=0.5")

```

从图中可以看出，p1=0.5时，双峰分布最明显

### 3.20

A compound Poisson process is a stochastic process $\{X(t), t \geq 0\}$ that can be represented as the random $\operatorname{sum} X(t)=\sum_{i=1}^{N(t)} Y_i, t \geq 0$, where $\{N(t), t \geq 0\}$ is a Poisson process and $Y_1, Y_2, \ldots$ are iid and independent of $\{N(t), t \geq 0\}$. Write a program to simulate a compound Poisson $(\lambda)$-Gamma process ( $Y$ has a Gamma distribution). Estimate the mean and the variance of $X(10)$ for several choices of the parameters and compare with the theoretical values. Hint: Show that $E[X(t)]=\lambda t E\left[Y_1\right]$ and $\operatorname{Var}(X(t))=\lambda t E\left[Y_1^2\right]$


```{r}
pg<-function(t,lambda,a,b,n=1000){
  x<-rep(0,n)
  for(i in 1:n){
    nt<-sum(rpois(t,lambda))
    x[i]<-sum(rgamma(nt,a,b))
  }
  return(x)
}
z<-pg(t=10,lambda=1,a=1,b=2,10000)
hist(z,main="compound Poisson(1)–Gamma process ")
mean(z)
var(z)
```

gamma(a,b)分布的均值为$\frac{a}{b}$，方差为$\frac{a}{b^2}$，所以gamma(1,2)分布的均值为1/2，方差为1/4，$\lambda=1$时$t\lambda E[Y_1]=5$与模拟得到的混合分布的均值相同，$t\lambda E[Y^2_1]=5$与模拟得到的混合分布的方差相同。

## HW 2

### 5.4

Write a function to compute a Monte Carlo estimate of the $Beta(3, 3)$ cdf,and use the function to estimate $F(x)$ for $x =0.1,0.2,...,0.9$ . Compare the estimates with the values returned by the pbeta function in R.

```{r}
set.seed(12345)
beta33<-function(num){
  m <- 1e4
  x <- runif(m, min=0, num)
  theta.hat <- mean(30*x^2*(1-x)^2)*num
  return(theta.hat)
}
x<-c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)
data<-matrix(rep(0,18),2)
for(i in 1:length(x)){
  data[1,i]<-beta33(x[i])
  data[2,i]<-pbeta(x[i],3,3)
}
round(data,4)
```

 $x=$|0.1|0.2|0.3|0.4|0.5|0.6|0.7|0.8|0.9
:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:
beta33|0.0086 |0.0574 |0.1629 |0.3132 |0.5019| 0.6923 |0.8407 |0.9406 |0.9862
pbeta|0.0086 |0.0579 |0.1631 |0.3174 |0.5000 |0.6826 |0.8369 |0.9421 |0.9914

从模拟结果可以看出，自己写的$Beta(3, 3)$cdf的蒙特卡洛估计函数与R中pbeta函数在$x =0.1,0.2,...,0.9$处得到的值极为相似

### 5.9

The Rayleigh density [156, (18.76)] is $f(x)= \frac x{σ^2} e^{-\frac {x^2}{2σ^2}}, x≥0,σ>0$ .Implement a function to generate samples from a Rayleigh(σ) distribution,using antithetic variables. What is the percent reduction in variance of $\frac{X+X'}2$ compared with $\frac{X_1+X_2}2$ for independent $X_1, X_2$?

```{r}
rayleigh1<- function(n, sigma) {
  u<-runif(n)
  x<-sigma*sqrt(-2*log(1-u))
  x1<-sigma*sqrt(-2*log(u))
  return((x+x1)/2)
}
rayleigh2<-function(n,sigma) {
  X1<-sigma*sqrt(-2*log(runif(n)))
  X2<-sigma*sqrt(-2*log(runif(n)))
  
  return((X1+X2)/2)
}
n <- 10000 
sigma<-1 
#对偶变量
s1<-rayleigh1(n, sigma)
#独立变量
s2<-rayleigh2(n, sigma)
var1<- var(s1)
var1
var2<- var(s2)
var2
(var2-var1)/var2

```
从模拟结果可以明确看出，对偶变量的方差小于独立变量的方差，方差减少了0.946左右


### 5.11

Find two importance functions $f_1$ and $f_2$ that are supported on (1,∞) and are ‘close’ to $g(x)= \frac{x^2}{\sqrt{2π}} e^{−\frac{x^2}2}, x>1$ .Which of your two importance functions should produce the smaller variance in estimating $\int_∞^1\frac{x^2}{\sqrt{2π}} e^{−\frac{x^2}2}dx$ by importance sampling? Explain.

```{r}
set.seed(12345)
g<-function(x){
  (x^2/sqrt(2*pi))*exp(-x^2/2)
}
f1_sample<-function(n){
  u <- runif(n)  
  samples <- qnorm(u * (1 - pnorm(1)) + pnorm(1))#逆变换
  g_values<-g(samples)
  f1_values<-dnorm(samples)/(1-pnorm(1))
  mean<-mean(g_values/f1_values)
  variance<-var(g_values/f1_values)
  return(list(mean,variance))
}
f2_sample<-function(n){
  u <- runif(n)
  samples <- qchisq(u * (1 - pchisq(1, df = 4)) + pchisq(1, df = 4), df = 4)#逆变换
  g_values <- g(samples)
  f2_values <-dchisq(samples,df = 4)/(1-pchisq(1,df = 4))
  mean<-mean(g_values/f2_values)
  variance <- var(g_values/f2_values)
  return(list(mean,variance))
}
n <- 10000
var_f1<-f1_sample(n)
var_f2<-f2_sample(n)
var_f1
var_f2
```
选择$f_1$为截断的正态分布，即正态分布大于1的部分，$f_1=\frac{1}{\sqrt{2π}} e^{−\frac{x^2}2}/F_1(1)$，其中$F_1$为正态分布的分布函数

选择$f_2$为截断的自由度为4的卡方分布$\chi^2(4)$，即卡方分布$\chi^2(4)$大于1的部分，$f_2=\frac{1}{4}xe^{−\frac{x}2}/F_2(1)$，其中$F_2$为卡方分布$\chi^2(4)$的分布函数

从模拟结果来看,$g(x)$在$(0,\infty)$上的积分约等于0.4，$f_1=\frac{1}{\sqrt{2π}} e^{−\frac{x^2}2}/F_1(1)$作为重要采样函数得到的估计的方差更小，猜测是$f_1$含有$e^{-\frac{x^2}{2}}$的结构，与$g(x)$更相似。


### Monte Carlo experiment
 
For $n = 10^4,2×10^4,4×10^4,6×10^4,8×10^4$, apply the fastsorting algorithm to randomly permuted numbers of $1,...,n$.
 
Calculate computation time averaged over 100 simulations,denoted by $a_n$.
 
Regress an on $t_n := nlog(n)$, and graphically show the results(scatter plot and regression line)

```{r}
library(ggplot2)
n<- c(10^4, 2*10^4, 4*10^4, 6*10^4, 8*10^4)
tn<- n* log(n)
an<- numeric(5)
set.seed(12345) 
#快速排序
quick_sort <- function(n) {
  if (length(n) < 2) {
    return(n)
  } else {
    x <- n[1]
    left<- n[n<x]
    right<- n[n >= x][-1] 
    return(c(quick_sort(left), x, quick_sort(right)))
  }
}

for (i in 1:length(n)) {
  n0<- n[i]
  #100次模拟
  times<- numeric(100)  
  for (j in 1:100) {
    n1 <- sample(1:n0, n0)  
    start<- Sys.time()  
    quick_sort(n1)  
    end<- Sys.time() 
    times[j] <- as.numeric(difftime(end, start, units = "secs"))  
  }
  an[i] <- mean(times)  
}

data <- data.frame(tn = tn, an = an)
model <- lm(an ~ tn, data = data)
summary(model)
ggplot(data, aes(x = tn, y = an)) +
  geom_point(color = 'blue') +  
  geom_smooth(method = "lm", col = "red") + 
  labs(title = "Regression of an on tn",
       x = "tn",
       y = "an")

```

## HW 3

### 6.6

Estimate the 0.025, 0.05, 0.95, and 0.975 quantiles of the skewness $\sqrt{b_1}$ under normality by a Monte Carlo experiment. Compute the standard error of the estimates from (2.14) using the normal approximation for the density (with exact variance formula). Compare the estimated quantiles with the quantiles of the large sample approximation $\sqrt{b_1} ≈ N(0,6/n)$.

```{r}
set.seed(12345)
n<-1000
skewness<-function(x) {
  m3<-mean((x-mean(x))^3)
  m2<-mean((x-mean(x))^2)
  return(m3/(m2^(3/2)))
}
b1_values<-replicate(n,{
  sn<-rnorm(n)  
  b1<-skewness(sn)
})
qmc<-quantile(b1_values,probs=c(0.025, 0.05, 0.95, 0.975))
qthe<-qnorm(c(0.025, 0.05, 0.95, 0.975),mean=0,sd=sqrt(6/n))
q<-data.frame(
  quantile=c(0.025, 0.05, 0.95, 0.975),
  monteCarlo=qmc,
  theoretical=qthe
)
print(q)

```


由(2.14)式知q分位数的方差为$var(\hat x_q)=\frac{q(1-q)}{nf(x_q)^2}$，所以偏度$\sqrt{b_1}$的q分位数的标准差分别为

```{r}
q_sd<-function(q){
  return(sqrt(q*(1-q)/n/dnorm(qnorm(q,0,sqrt(6/n)),0,sqrt(6/n))))
}
se_mc<-q_sd(c(0.025, 0.05, 0.95, 0.975))
print(se_mc)
```



### 6.B

Tests for association based on Pearson product moment correlation ρ,Spearman’s rank correlation coefficient ρs, or Kendall’s coefficient τ, are implemented in cor.test. Show (empirically) that the nonparametric tests based on ρs or τ are less powerful than the correlation test when the sampled distribution is bivariate normal. Find an example of an alternative (a bivariate distribution (X,Y) such that X and Y are dependent) such that at least oneof the nonparametric tests have better empirical power than the correlation test against this alternative.

通过下面的模拟实验可观察到，双变量正态分布pearson检验的功效高于非参数的spearman检验和kendall检验

```{r}
set.seed(12345)
library(MASS)
n <- 100
mu <- c(0, 0) 
Sigma <- matrix(c(1, 0.2, 0.2, 1), 2, 2) 
data_normal <- mvrnorm(n, mu = mu, Sigma = Sigma)
x <- data_normal[, 1]
y <- data_normal[, 2]
cor_pearson <- cor.test(x, y, method = "pearson")
cor_spearman <- cor.test(x, y, method = "spearman")
cor_kendall <- cor.test(x, y, method = "kendall")
cat("双变量正态分布下的相关性检验:\n")
print(cor_pearson)
print(cor_spearman)
print(cor_kendall)
calculate_power <- function(n, rho, alpha = 0.05, method = c("pearson", "spearman", "kendall"), reps = 1000) {
  reject_count <- 0
  for (i in 1:reps) {
    Sigma <- matrix(c(1, rho, rho, 1), 2, 2) 
    data <- mvrnorm(n, mu = c(0, 0), Sigma = Sigma)
    x <- data[, 1]
    y <- data[, 2]
    test_result <- cor.test(x, y, method = method)
    if (test_result$p.value < alpha) {
      reject_count <- reject_count + 1
    }
  }
  return(reject_count / reps)
} 
rho <- 0.3
alpha <- 0.05  
reps <- 1000 
power_pearson <- calculate_power(n, rho, alpha, method = "pearson", reps = reps)
power_spearman <- calculate_power(n, rho, alpha, method = "spearman", reps = reps)
power_kendall <- calculate_power(n, rho, alpha, method = "kendall", reps = reps)
cat("皮尔逊检验的功效:", power_pearson, "\n")
cat("斯皮尔曼检验的功效:", power_spearman, "\n")
cat("肯德尔检验的功效:", power_kendall, "\n")
```

通过下面的模拟实验可以观察到，双变量t分布的pearson检验的功效低于非参数的spearman检验和kendall检验

```{r}
#反例
n<-100
x_alt <- rnorm(n)
y_alt <- x_alt^2 + rnorm(n)
cor_pearson_alt <- cor.test(x_alt, y_alt, method = "pearson")
cor_spearman_alt <- cor.test(x_alt, y_alt, method = "spearman")
cor_kendall_alt <- cor.test(x_alt, y_alt, method = "kendall")
cat("\n双变量t分布的相关性检验:\n")
print(cor_pearson_alt)
print(cor_spearman_alt)
print(cor_kendall_alt)
calculate_power1 <- function(n,rho,df,alpha = 0.05, method = c("pearson", "spearman", "kendall"), reps = 1000) {
  reject_count <- 0
  for (i in 1:reps) {
    Sigma<-matrix(c(1,rho,rho,1),2,2)
    data_t <- mvtnorm::rmvt(n, sigma = Sigma, df = df)
    x_alt <- data_t[, 1]
    y_alt <- data_t[, 2]
    test_result <- cor.test(x_alt, y_alt, method = method)
    if (test_result$p.value < alpha) {
      reject_count <- reject_count + 1
    }
  }
  return(reject_count / reps)
} 
alpha <- 0.05  
reps <- 1000 
rho<-0.3
df<-2
power_pearson <- calculate_power1(n,rho,df,alpha,method = "pearson", reps = reps)
power_spearman <- calculate_power1(n,rho,df,alpha,method = "spearman", reps = reps)
power_kendall <- calculate_power1(n,rho,df,alpha,method = "kendall", reps = reps)
cat("皮尔逊检验的功效:", power_pearson, "\n")
cat("斯皮尔曼检验的功效:", power_spearman, "\n")
cat("肯德尔检验的功效:", power_kendall, "\n")
```



If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. We want to know if the powers are different at 0.05 level.
 
What is the corresponding hypothesis test problem?
 
What test should we use? Z-test, two-sample t-test, paired-t test or McNemar test? Why?
 
Please provide the least necessary information for hypothesis testing

可以使用两样本Z检验，与两样本t检验相比，Z检验更适合大样本；数据是一样的不涉及配对数据，所以选择使用Z检验

最少需要的信息：两种方法的功效估计值，每种方法的实验次数，显著性水平$\alpha$

## classwork

```{r}
d1<- c(-2.961, 0.478, -0.391, -0.869, -0.460, -0.937, 0.779, -1.409, 0.027, -1.569)
d2<- c(1.608, 1.009,  0.878,  1.600, -0.263,  0.680, 2.280,  2.390, 1.793,  8.091,1.468)
B<-1e4
set.seed(12345)
thetastar<-numeric(B)
theta1<-mean(d1)
theta2<-mean(d2)
mean<-(theta1-theta2)
for(b in 1:B){
 xstar1<-sample(d1,length(d1),replace=TRUE)
 xstar2<-sample(d2,length(d2),replace=TRUE)
 thetastar[b]<-mean(xstar1)-mean(xstar2)
 }
round(c(theta.boot=mean(thetastar),theta=mean,bias=mean(thetastar)-mean,se.boot=sd(thetastar),se.samp=sqrt(var(d1)/(length(d1))+var(d2)/(length(d2)))),5)
```
  
## HW 4

Of N =1000 hypotheses, 950 are null and 50 are alternative.The p-value under any null hypothesis is uniformly distributed(use runif), and the p-value under any alternative hypothesis follows the beta distribution with parameter 0.1 and 1 (use rbeta). Obtain Bonferroni adjusted p-values and B-H adjusted p-values. Calculate FWER, FDR, and TPR under nominal level α =0.1 for each of the two adjustment methods based on m=10000 simulation replicates. You should output the 6 numbers (3 ) to a 3×2 table (column names: Bonferroni correction, B-H correction; row names: FWER, FDR, TPR).Comment the results.


```{r}
set.seed(12345)  
N<-1000      
null_hypotheses<-950
alt_hypotheses<-50
m<-10000
alpha<-0.1
calculate_metrics<-function(p_values){
  reject_null<-p_values<alpha
  FWER<-mean(reject_null[1:null_hypotheses]) 
  TPR<-mean(reject_null[(null_hypotheses+1):N]) 
  FDR<-mean(reject_null)  
  return(c(FWER,FDR,TPR))
}
results<-matrix(0,nrow=m,ncol=6)
for(i in 1:m){
  p_values<-c(runif(null_hypotheses),rbeta(alt_hypotheses,0.1,1))
  bonferroni_adjusted<-p.adjust(p_values,method="bonferroni")
  bonferroni_metrics<-calculate_metrics(bonferroni_adjusted)
  bh_adjusted<-p.adjust(p_values,method="BH")
  bh_metrics<-calculate_metrics(bh_adjusted)
  results[i, ]<-c(bonferroni_metrics,bh_metrics)
}
colMeans(results)
```
table|Bonferroni correction|B-H correction
:-:|:-:|:-:
FWER|9.705263e-05|0.019929000
FDR|3.967360e-01|0.003237368
TPR|3.112860e-02|0.561062000

从结果可以看出Bonferroni修正，FWER较低；B-H修正，FDR较低

### 7.4

Refer to the air-conditioning data set aircondit provided in the boot package. The 12 observations are the times in hours between failures of airconditioning equipment [63, Example 1.1]:$$3, 5,7,18,43,85,91,98,100,130,230,487.$$Assume that the times between failures follow an exponential model Exp(λ).Obtain the MLE of the hazard rate λ and use bootstrap to estimate the bias and standard error of the estimate.

```{r}
set.seed(12345) 
library(boot)
times<-c(3,5,7,18,43,85,91,98,100,130,230,487)
mle<-1/mean(times)
bootstrap_lambda<-function(data,i) {
  sample<-data[i]
  return(1/mean(sample))
}
results<-boot(data=times,statistic=bootstrap_lambda,R=1000)
bias<-mean(results$t)-mle
std<-sd(results$t)
mle
bias
std
```
$\lambda$的MLE为0.00925212，使用自助法得到的偏差为0.001305434，标准差为0.004258588

### 7.5

Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the mean time between failures 1/λ by the standard normal, basic, percentile,and BCa methods. Compare the intervals and explain why they may differ.

```{r}
set.seed(12345)
library(boot)
alpha<-0.05
bootstrap_lambda<-function(data,i) {
  sample<-data[i]
  return(mean(sample))
}
results<-boot(data=times,statistic=bootstrap_lambda,R=1000)
mean_boot<-mean(results$t)
std<-sd(results$t)
z<-qnorm(1-alpha/2)
normal<-c(mean_boot-z*std,mean_boot+z*std)
basic<-c(2*mean_boot-quantile(results$t,1-alpha/2),2*mean_boot-quantile(results$t,alpha/2))
percentile<-quantile(results$t, c(alpha/2,1-alpha/2))
bca<-boot.ci(results, type = "bca")$bca[4:5]
normal
basic
percentile
bca
```

利用标准正态得到置信区间[34.16154 181.90679]

basic方法得到的置信区间[25.73292 168.90792]

percentile方法得到的置信区间为[47.16042 190.33542]

bca方法得到的置信区间为[56.85638 231.84657]

## HW 5

### 7.8

Efron and Tibshirani discuss the scor (bootstrap)test score data on 88 students who took examinations in five subjects The five-dimensional scores data have a 5 × 5 covariance matrix Σ,with positive eigenvalues $λ_1 > ···>λ_5$. In principal components analysis,$\theta=\frac{λ_1}{\sum^5_{j=1}λ_j}$measures the proportion of variance explained by the first principal component. Let$\hatλ_1 > ···>\hatλ_5$be the eigenvalues of $\hatΣ$, where $\hatΣ$is the MLE of Σ.Compute the sample estimate $\hat\theta=\frac{\hatλ_1}{\sum^5_{j=1}\hatλ_j}$ of θ. Use bootstrap to estimate the bias and standard error of $\hatθ$. Obtain the jackknife estimates of bias and standard error of $\hat\theta$ .

```{r}
library(bootstrap)
sigma<-cov(scor)
lambda<-eigen(sigma)$val
lambda1<-lambda[which.max(lambda)]
theta<-lambda1/sum(lambda)
n<-nrow(scor)
theta.jack<-numeric(n)
for(i in 1:n){
  sigma<-cov(scor[-i,])
  lambda<-eigen(sigma)$val
  theta.jack[i]<-lambda[which.max(lambda)]/sum(lambda)
}
bias<-(n-1)*(mean(theta.jack)-theta)
se<-sqrt((n-1)*mean(theta.jack-mean(theta.jack)^2))
bias
se
```
### 7.10

In Example 7.18, leave-one-out (n-fold) cross validation was used to select the best fitting model. Repeat the analysis replacing the Log-Log model with a cubic polynomial model. Which of the four models is selected by the cross validation procedure? Which model is selected according to maximum adjusted $R^2$?

```{r}
 library(DAAG)
 n <- length(ironslag$magnetic) 
 e1 <- e2 <- e3 <- e4<-e5 <- numeric(n)
 for (k in 1:n) {
 y <- ironslag$magnetic[-k]
 x <- ironslag$chemical[-k]
 J1 <- lm(y ~ x)
 yhat1 <- J1$coef[1] + J1$coef[2] * ironslag$chemical[k]
 e1[k] <- ironslag$magnetic[k]- yhat1
 J2 <- lm(y ~ x + I(x^2))
 yhat2 <- J2$coef[1] + J2$coef[2] * ironslag$chemical[k] +
 J2$coef[3] * ironslag$chemical[k]^2
 e2[k] <- ironslag$magnetic[k]- yhat2
 J3 <- lm(log(y) ~ x)
 logyhat3 <- J3$coef[1] + J3$coef[2] * ironslag$chemical[k]
 yhat3 <- exp(logyhat3)
 e3[k] <- ironslag$magnetic[k]- yhat3
 J4 <- lm(log(y) ~ log(x))
 logyhat4 <- J4$coef[1] + J4$coef[2] * log(ironslag$chemical[k])
 yhat4 <- exp(logyhat4)
 e4[k] <- ironslag$magnetic[k]- yhat4
 J5<-lm(y~x+I(x^2)+I(x^3))
 yhat5<-J5$coef[1]+J5$coef[2]*ironslag$chemical[k]+J5$coef[3]*ironslag$chemical[k]^2+J5$coef[4]*ironslag$chemical[k]^3
 e5[k] <- ironslag$magnetic[k]- yhat5
 }
 c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2),mean(e5^2))
 lm(ironslag$magnetic ~ ironslag$chemical + I(ironslag$chemical^2))
```

从模拟结果来看，将双对数模型换为三次多项式模型，交叉验证后还是二次函数得到的误差最小$\hat Y=24.49262−1.39334X +0.05452X^2.$

```{r}
library(DAAG)
 n <- length(ironslag$magnetic) 
 r1 <- r2 <- r3 <- r4<-r5 <- numeric(n)
 for (k in 1:n) {
 y <- ironslag$magnetic[-k]
 x <- ironslag$chemical[-k]
 J1 <- lm(y ~ x)
 r1[k] <- summary(J1)$adj.r.squared
 J2 <- lm(y ~ x + I(x^2))
 r2[k] <- summary(J2)$adj.r.squared
 J3 <- lm(log(y) ~ x)
 r3[k] <- summary(J3)$adj.r.squared
 J4 <- lm(log(y) ~ log(x))
 r4[k] <- summary(J4)$adj.r.squared
 J5<-lm(y~x+I(x^2)+I(x^3))
 r5[k] <- summary(J5)$adj.r.squared
 }
 c(mean(r1^2), mean(r2^2), mean(r3^2), mean(r4^2),mean(r5^2))
```
从模拟结果来看，交叉验证后通过$R^2$的最大调整为0.3326365，还是二次函数得到的最大$\hat Y=24.49262−1.39334X +0.05452X^2.$所以综合来看，二次多项式函数拟合最好。

### 8.1

Implement the two-sample Cram´er-von Mises test for equal distributions as a permutation test. Apply the test to the data in Examples 8.1 and 8.2.

```{r}
library(twosamples)
 x <- as.vector(chickwts$weight[chickwts$feed == "soybean"])
 y <- as.vector(chickwts$weight[chickwts$feed == "linseed"])
 R <- 999
 z <- c(x, y)
 K <- 1:26
 reps <- numeric(R)
 t0 <- cvm_test(x, y)[1]
 for (i in 1:R) {
 k <- sample(K, size = 14, replace = FALSE)
 x1 <- z[k]
 y1 <- z[-k]
 reps[i] <- cvm_test(x1, y1)[1]
 }
 p <- mean(c(t0, reps) >= t0)
 p
```

从模拟结果来看，p值比较大，不能拒绝零假设。

### 8.2

Implement the bivariate Spearman rank correlation test for independence [255] as a permutation test. The Spearman rank correlation test statistic can be obtained from function cor with method = "spearman". Compare the achieved significance level of the permutation test with the p-value reported by cor.test on the same samples.

```{r}
x <- as.vector(chickwts$weight[chickwts$feed == "sunflower"])
 y <- as.vector(chickwts$weight[chickwts$feed == "linseed"])
 R <- 999
 z <- c(x, y)
 K <- 1:24
 reps <- numeric(R)
 t0 <- cor(x, y,method="spearman")
 for (i in 1:R) {
 k <- sample(K, size = 12, replace = FALSE)
 x1 <- z[k]
 y1 <- z[-k]
 reps[i] <- cor(x1, y1,method="spearman")
 }
 p <- mean(c(t0, reps) >= t0)
 p
 cor.test(x, y,method="spearman")$p.value
```
从模拟结果来看，置换检验和cor.test的p值都比较大，不能拒绝零假设。置换检验的p值稍小一点

## HW 6

### 9.3

Use the Metropolis-Hastings sampler to generate random variables from a standard Cauchy distribution. Discard the first 1000 of the chain, and compare the deciles of the generated observations with the deciles of the standard Cauchy distribution (see qcauchyor qt with df=1). Recall that a Cauchy(θ,η) distribution has density function $f(x)=\frac1{θπ(1 +[(x −η)/θ]^2)} ,−∞<x<∞, θ>0$.The standard Cauchy has the Cauchy(θ =1,η= 0) density. (Note that the standard Cauchy density is equal to the Student t density with one degree of freedom.)




```{r}
set.seed(1234)
library(ggplot2)
metropolis_hastings <- function(n) {
  x<-numeric(n)
  x[1]<-rnorm(1,0,sd)  
  for (i in 2:n) {
    y <- rnorm(1,mean=x[i-1],sd)
    ratio<-dcauchy(y)*dnorm(x[i-1],mean=y,sd)/(dcauchy(x[i-1])*dnorm(y,mean=x[i-1],sd))
    if (runif(1)>ratio) {
      x[i]<-x[i-1]
    } else {
      x[i]<-y
    }
  }
  return(x)
}
gelman_rubin <- function(chains) {
  m <- ncol(chains)  
  n <- nrow(chains)
  means <- colMeans(chains)
  overall_mean <- mean(means)
  B <- n * var(means)  #序列间方差
  W <- mean(apply(chains, 2, var))  #序列内方差
  var_hat <- (n - 1) / n * W + B / n
  R_hat <- (var_hat / W)
  return(R_hat)
}
n_samples <- 10000
n_chains <- 4
sd<-1
k<-1
chains <- matrix(NA, n_samples, n_chains)
for (i in 1:n_chains) {
  chains[, i] <- metropolis_hastings(n_samples)
}
R_hat<-numeric(n_samples)
repeat {
  k<-k+1
  if(k>n_samples){
    break
  }
  R_hat[k] <- gelman_rubin(chains[1:k,])
}
plot(R_hat,type='l',ylim=c(0,2))
abline(h=1.2)

```

采用gelman_rubin方法画图观察到，$\hat R$起初在1.0-1.3之间震荡，在三千多之后稳定小于1.2，维持在1,0附近


```{r}
chains <- chains[-(1:1000), ]
generated_deciles <- apply(chains, 2, quantile, probs = seq(0.1, 0.9, by = 0.1))
theoretical_deciles <- qcauchy(seq(0.1, 0.9, by = 0.1))
comparison <- data.frame(
  Decile = seq(0.1, 0.9, by = 0.1),
  Generated = rowMeans(generated_deciles),
  Theoretical = theoretical_deciles
)
print(comparison)
ggplot(comparison, aes(x = Decile)) +
  geom_line(aes(y = Generated, color = "Generated"), linewidth = 1) +
  geom_line(aes(y = Theoretical, color = "Theoretical"), linewidth = 1) +
  labs(title = "Decile Comparison: Generated vs Theoretical Cauchy Distribution",
       x = "Decile", y = "Value") +
  scale_color_manual(values = c("Generated" = "blue", "Theoretical" = "red")) +
  theme_minimal()

```

以$N(X_t,1)$为建议分布，去掉前1000项观测值的十分位数与标准柯西分布的十分位数非常接近，在0.1或0.9分位数处可能会差距较大


### 9.8

This example appears in [40]. Consider the bivariate density$f(x,y) ∝ nx y^{(x+a−1)}(1−y)^{n−x+b−1}, x=0,1,...,n, 0 ≤ y ≤ 1.$ It can be shown (see e.g. [23]) that for fixed a,b,n, the conditional distributions are Binomial(n,y) and Beta(x+a,n−x+b). Use the Gibbs sampler to generate a chain with target joint density f(x,y).

```{r}
library(ggplot2)
gibbs<- function(n_samples) {
  n <- 10 
  x <- numeric(n_samples)
  y<- numeric(n_samples)
  y[1]<-0.5
  x[1]<-0.5
  for (i in 2:n_samples) {
    x1<-x[i-1]
    y1<-y[i-1]
    x1<- rbinom(1, n, y1)
    x[i]<-x1
    y[i]<- rbeta(1, x1+a,n-x1+ b)
  }
  return(data.frame(x,y))
}
    
gelman_rubin <- function(chains) {
  m <- ncol(chains)  
  n <- nrow(chains)
  means <- colMeans(chains)
  overall_mean <- mean(means)
  B <- n * var(means)  #序列间方差
  W <- mean(apply(chains, 2, var))  #序列内方差
  var_hat <- (n - 1) / n * W + B / n
  R_hat <- (var_hat / W)
  return(R_hat)
}
a <- 2     
b <- 3 
n_samples <- 10000
n_chains <- 4
k<-1
n<-10
chainsx<-chainsy<-matrix(NA, n_samples, n_chains)
for (i in 1:n_chains) {
  chainsx[,i]<-gibbs(n_samples)$x
  chainsy[,i]<-gibbs(n_samples)$y
}
R_hatx<-R_haty<-rep(0,n_samples)
repeat {
  k<-k+1
  if(k>n_samples){
    break
  }
  R_hatx[k] <- gelman_rubin(chainsx[1:k,])
  R_haty[k] <- gelman_rubin(chainsy[1:k,])
}
plot(R_hatx[1:n_samples],type='l')
plot(R_haty[1:n_samples],type='l')
par(mfrow=c(1, 2))
hist(chainsx[,1],breaks = seq(-0.5, n + 0.5, by = 1), probability = TRUE,main = "Histogram of x samples",xlab = "x", col = "lightblue", xlim = c(0, n))
hist(chainsy[,1], breaks = 30, probability = TRUE,main = "Histogram of y samples",xlab = "y", col = "lightgreen", xlim = c(0, 1))
```

当n比较大时，$\hat R$较容易小于1.2，当n>500后，$\hat R$比较稳定的趋于1


### stationarity

Algorithm (continuous situation)

 Target pdf: f (x).
 
 Replace i and j with s and r.
 
 Proposal distribution (pdf): g(r|s).
 
 Acceptance probability: $α(s,r) = min\{\frac{f(r)g(s|r)}{f(s)g(r|s)},1\}$
 
 Transition kernel (mixture distribution):
 
 $K(r,s) = I(s\neq r)α(r,s)g(s|r) + I(s = r)[1 − \int α(r,s)g(s|r)]$.
 
 Stationarity: $K(s,r)f (s) = K(r,s)f (r)$.
 
### proof

s=r时，$K(r,s)=1-\int \alpha(r,s)g(s|r)=1-\int min\{\frac{f(s)g(r|s)}{f(r)g(s|r)},1\}g(s|r)=1-g(s)=1-g(r)=K(s,r)$

$s\neq r$时，$K(s,r)=\alpha(r,s)g(r|s)f(s)=min\{\frac{f(r)g(s|r)}{f(s)g(r|s)},1\}g(r|s)f(s)=min\{f(r)g(s|r),f(s)g(r|s)\}=min\{f(s)g(r|s),f(r)g(s|r)\}=K(r,s)$

## HW 7

## 11.3

### (a)

Write a function to compute the kth term in $$\sum^{\infty}_{K=0}\frac{(-1)^k}{k!2^k}\frac{||a||^{2k+2}}{(2k+1)(2k+2)}\frac{\Gamma(\frac{d+1}{2})\Gamma(k+\frac32)}{\Gamma(k+\frac d2+1)}$$ where d ≥ 1 is an integer, a is a vector in Rd,and · denotes the Euclidean norm. Perform the arithmetic so that the coefficients can be computed for (almost) arbitrarily large k and d. (This sum converges for all $a ∈ R^d$)

```{r}
fk<-function(k,a,d){
  x<-(-1)^k/2^k/factorial(k)
  b<-norm(a,"2")^(2*k+2)/(2*k+1)/(2*k+2)
  c<-exp(lgamma((d+1)/2)+lgamma(k+3/2)-lgamma(k+d/2+1))
  return(x*b*c)
}
```

### (b)

Modify the function so that it computes and returns the sum.

```{r}
f<-function(a,d){
  sumk<-0
  for(k in 0:50){
    x<-(-1)^k/2^k/factorial(k)
    b<-norm(a,"2")^(2*k+2)/(2*k+1)/(2*k+2)
    c<-exp(lgamma((d+1)/2)+lgamma(k+3/2)-lgamma(k+d/2+1))
    sumk<-sumk+x*b*c
  }
  return(sumk)
}
```

### (c)

Evaluate the sum when $a =(1,2)^T$.

```{r}
f<-function(a,d){
  sumk<-0
  for(k in 0:100){
    x<-(-1)^k/2^k/factorial(k)
    b<-norm(a,"2")^(2*k+2)/(2*k+1)/(2*k+2)
    c<-exp(lgamma((d+1)/2)+lgamma(k+3/2)-lgamma(k+d/2+1))
    sumk<-sumk+x*b*c
  }
  return(sumk)
}
a<-c(1,2)
d<-1
f(a,d)
```

## 11.5

Write a function to solve the equation$$\frac{2\Gamma(\frac k2)}{\sqrt{\pi(k-1)}\Gamma(\frac{k-1}2)}\int^{c_{k-1}}_0(1+\frac{u^2}{k-1})^{-k/2}du=\frac{2\Gamma(\frac {k+1}2)}{\sqrt{\pi k}\Gamma(\frac{k}2)}\int^{c_{k}}_0(1+\frac{u^2}{k})^{-(k+1)/2}du$$ for a,where $c_k=\sqrt{\frac{a^2k}{k+1-a^2}}$ 


```{r}
left_integral<-function(ck1, k){
  integrand<-function(u) (1+u^2/(k-1))^(-k/2)
  integrate(integrand,0,ck1)$value
}
right_integral<-function(ck, k){
  integrand<-function(u) (1 + u^2/k)^(-(k + 1) / 2)
  integrate(integrand,0,ck)$value
}
equation<-function(a, k) {
  ck1<-sqrt(a^2*(k-1)/(k-a^2))
  ck<-sqrt(a^2*k/(k+1-a^2))
  left<-(2*exp(lgamma((k)/2)-lgamma((k-1)/2))/sqrt(pi*(k-1)))*left_integral(ck1, k)
  right<-(2*exp(lgamma((k+1)/2)-lgamma(k/2))/sqrt(pi*k))*right_integral(ck, k)
  return(left-right)
}
solve_for_a <- function(k) {
  solution <- uniroot(equation,interval=c(0.1,sqrt(k)-1),k=k)
  return(solution$root)
}
k<-10
a_solution<-solve_for_a(k)
a_solution
```


Suppose T1,...,Tn are i.i.d. samples drawn from the exponential distribution with expectation λ. Those values greater than τ are not observed due to right censorship, so that the observed values are $Y_i = T_iI(T_i ≤ τ) + τI(T_i > τ),i = 1,...,n$. Suppose τ = 1 and the observed $Y_i$ values are as follows:0.54, 0.48,0.33,0.43,1.00,1.00,0.91,1.00,0.21,0.85 Use the E-M algorithm to estimate λ, compare your result with the observed data MLE (note: $Y_i$ follows a mixture distribution)

E：$E[T_i|T_i>\tau,\lambda]=\tau+\lambda$
  
  $Q^{(t)}=\sum_{Y_i<\tau}Y_i+\sum_{Y_i=\tau}(\tau+\lambda_t)$
  
  M:$\lambda_{t+1}=\frac{Q^{(t)}}{n}$
  
  
```{r}
Y<-c(0.54, 0.48, 0.33, 0.43, 1.00, 1.00, 0.91, 1.00, 0.21, 0.85)
tau<-1
n<-length(Y)
epsilon<-0.0001
lambda<-1
repeat {
  lambda_old<-lambda
  Q_t<-sum(Y[Y<tau])+sum((tau+lambda)*(Y==tau))
  lambda<-Q_t/n
  if (abs(lambda-lambda_old)< epsilon){
    break
  }
}
lambda

```


由均值为$\lambda$可知，$f(T)=\frac1{\lambda}e^{-\frac t{\lambda}}$
  
  $f(y)=\Pi^n_{i=1}[\frac1{\lambda}e^{-\frac {y_i}{\lambda}}I\{0<y_i<1\}+e^{-\frac{1}{\lambda}}I\{y_i=1\}]$
  
  $l(y)=\sum^n_{i=1}[(-\frac1{\lambda}y_i-log(\lambda))I\{0<y_i<1\}-\frac1{\lambda}I\{y_i=1\}]$
  
  令$\frac{\partial l(y)}{\partial y}=0$推出

$\lambda=\frac{\sum^n_{i=1}y_iI\{0<y_i<1\}+\sum^n_{i=1}I\{y_i=1\}}{\sum^n_{i=1}I\{0<y_i<1\}}$
  
```{r}
lambdamle<-(sum(Y))/sum(Y<1)
lambdamle
```

可以看到两种算法得到的结果非常接近

## HW 8

### 11.7

Use the simplex algorithm to solve the following problem.
Minimize 4x +2y +9z subject to
 
 2x+y+z≤2
 
 x−y+3z≤3
 
 x≥0,y≥0,z≥0.

```{r}
library(lpSolve)
ob<-c(4, 2, 9)
con<-matrix(c(
  2, 1, 1,  
  1, -1, 3  
),nrow=2, byrow=TRUE)
rhs<-c(2, 3)
dir<-c("<=","<=")
sol<-lp("min",ob,con,dir,rhs)
sol$solution 
sol$objval   
```


### 3

Use both for loops and lapply() to fit linear models to the mtcars using the formulas stored in this list: 

formulas <- list(
 mpg ~ disp,
 mpg ~ I(1 / disp),
 mpg ~ disp + wt,
 mpg ~ I(1 / disp) + wt
 )
 
```{r}
formulas<-list(
  mpg~disp,
  mpg~I(1/disp),
  mpg~disp+wt,
  mpg~I(1/disp)+wt
)
forloop3<-list()
for (i in seq_along(formulas)) {
  forloop3[[i]]<-lm(formulas[[i]],data=mtcars)
}
forloop3
lapply3<- lapply(formulas, function(f) lm(f, data = mtcars))
lapply3
```



### 4

Fit the model mpg ~ disp to each of the bootstrap replicates of mtcars in the list below by using a for loop and lapply().Can you do it without an anonymous function?

bootstraps <- lapply(1:10, function(i) {
 rows <- sample(1:nrow(mtcars), rep = TRUE)
 mtcars[rows, ]
 })
 
```{r}
set.seed(12345) 
bootstraps<-lapply(1:10,function(i){
  rows<-sample(1:nrow(mtcars),rep=TRUE)
  mtcars[rows, ]
})
forloop4<-list()
for(i in seq_along(bootstraps)){
  forloop4[[i]]<-lm(mpg~disp,data=bootstraps[[i]])
}
forloop4
fit<-function(data){
  lm(mpg~disp,data=data)
}
lapply4<-lapply(bootstraps,fit)
lapply4
```
 

### 5

For each model in the previous two exercises, extract R2 using the function below.
 
rsq <- function(mod) summary(mod)$r.squared

```{r}
rsq<-function(mod) summary(mod)$r.squared
rsqfor3<-numeric(length(forloop3))
for(i in seq_along(forloop3)){
  rsqfor3[i]<-rsq(forloop3[[i]])
}
rsqfor3
rsqlapply3<-sapply(lapply3, rsq)
rsqlapply3
```

```{r}
rsq<-function(mod) summary(mod)$r.squared
rsqfor4<-numeric(length(forloop4))
for(i in seq_along(forloop4)){
  rsqfor4[i]<-rsq(forloop4[[i]])
}
rsqfor4
rsqlapply4<-sapply(lapply4, rsq)
rsqlapply4
```


### 3

The following code simulates the performance of a t-test for non-normal data. Use sapply() and an anonymous function to extract the p-value from every trial.
 
trials <- replicate(
 100,
 t.test(rpois(10, 10), rpois(7, 10)),
 simplify = FALSE
 )

Extra challenge: get rid of the anonymous function by using [[ directly.

```{r}
trials<-replicate(
  100,
  t.test(rpois(10,10),rpois(7,10)),
  simplify=FALSE
)
p_values<-sapply(trials, function(t) t$p.value)
p_values
p_values<-sapply(trials, `[[`, "p.value")
p_values
```


### 6

Implement a combination of Map() and vapply() to create an lapply() variant that iterates in parallel over all of its inputs and stores its outputs in a vector (or a matrix). What arguments should the function take?

```{r}
lapply_variant<-function(FUN,...,FUN.VALUE){
  inputs<-list(...) 
  Map(FUN, ...)|>vapply(identity,FUN.VALUE=FUN.VALUE)
}
x<-1:10
y<-11:20
result<-lapply_variant(function(a,b) a+b,x,y,FUN.VALUE=numeric(1))
print(result) 
result_matrix<-lapply_variant(function(a,b) c(sum=a+b,product=a*b),x,y,FUN.VALUE=numeric(2))
print(result_matrix)
```


### 4

Make a faster version of chisq.test() that only computes the chi-square test statistic when the input is two numeric vectors with no missing values. You can try simplifying chisq.test() or by coding from the mathematical definition (http://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test).

```{r warning=FALSE}
fast_chisq1<-function(x, y){
  con_table<-table(x,y)
  row<-rowSums(con_table)
  col<-colSums(con_table)
  grand<-sum(con_table)
  expected<-outer(row,col)/grand
  observed<-as.numeric(con_table)
  expected<-as.numeric(expected)
  chisq_stat<-sum((observed-expected)^2/expected)
  return(chisq_stat)
}
fast_chisq2<-function(x,y){
  con_table<-table(x,y)
  E<-outer(rowSums(con_table),colSums(con_table))/sum(con_table)
  O<-con_table
  chisq_stat<-sum((O-E)^2/E)
  return(chisq_stat)
}
x<-c(1,2,2,3,1,3,2)
y<-c(2,2,3,1,1,3,2)
chisq_fast1<-fast_chisq1(x,y)
print(chisq_fast1)
chisq_fast2<-fast_chisq2(x,y)
print(chisq_fast2)
chisq_original<-chisq.test(table(x,y))$statistic
print(chisq_original)
```

### 5

Can you make a faster version of table() for the case of an input of two integer vectors with no missing values? Can you use it to speed up your chi-square test

```{r warning=FALSE}
fast_table<-function(x, y){
  x_levels<-sort(unique(x))
  y_levels<-sort(unique(y))
  result<-matrix(0L,nrow=length(x_levels),ncol=length(y_levels),dimnames=list(x_levels,y_levels))
  for(i in seq_along(x)){
    result[as.character(x[i]),as.character(y[i])]<-result[as.character(x[i]),as.character(y[i])]+1
  }
  return(result)
}
fast_chisq<-function(x,y){
  con_table<-fast_table(x, y)
  row<-rowSums(con_table)
  col<-colSums(con_table)
  grand<-sum(con_table)
  expected<-outer(row,col)/grand
  observed<-as.numeric(con_table)
  expected<-as.numeric(expected)
  chisq_stat<-sum((observed-expected)^2/expected)
  return(chisq_stat)
}
x<-as.integer(c(1,2,2,3,1,3,2))
y<-as.integer(c(2,2,3,1,1,3,2))
table_fast<-fast_table(x,y)
table_fast
chisq_fast<-fast_chisq(x, y)
chisq_fast
table_original<-table(x, y)
chisq_original<-chisq.test(table(x, y))$statistic
chisq_original
```

## HW 9

### 9.8

This example appears in [40]. Consider the bivariate density $f(x,y) ∝ C^x_ny^{(x+a−1)}(1−y)^{(n−x+b−1)}, x=0,1,...,n, 0 ≤ y ≤ 1.$It can be shown (see e.g. [23]) that for fixed a,b,n, the conditional distributions are Binomial(n,y)andBeta(x+a,n−x+b). Use the Gibbs sampler to generate a chain with target joint density f(x,y).

```{r}
library(Rcpp)
cppFunction("
NumericMatrix gibbs_sampler(int n,double a,double b,int N,int init_x,double init_y) {
  NumericMatrix samples(N,2);
  int x=init_x;
  double y=init_y;
  for(int i=0;i<N;i++){
    x=R::rbinom(n,y);
    y=R::rbeta(x+a,n-x+b);
    samples(i,0)=x;
    samples(i,1)=y;
  }
  return samples;
}
")
n<-10
a<-2
b<-3
N<-10000
init_x<-0.5
init_y<-0.5
samples<-gibbs_sampler(n,a,b,N,init_x,init_y)
par(mfrow=c(1,2))
plot(samples[,2],type="l",xlab="Iteration",ylab="y")
hist(samples[,2],breaks=30,main="y",xlab="y")
```

### 2

Compare the corresponding generated random numbers with those by the R function you wrote using the function “qqplot”.

```{r}
gibbs<-function(N){
  n<-10 
  x<-numeric(N)
  y<-numeric(N)
  y[1]<-0.5
  x[1]<-0.5
  for(i in 2:N){
    x1<-x[i-1]
    y1<-y[i-1]
    x1<-rbinom(1,n,y1)
    x[i]<-x1
    y[i]<-rbeta(1,x1+a,n-x1+b)
  }
  return(data.frame(x,y))
}
n<-10
a<-2
b<-3
N<-10000
init_x<-0.5
init_y<-0.5
samplescpp<-gibbs_sampler(n,a,b,N,init_x,init_y)  
samplesR<-gibbs(N)
qqplot(samplescpp[,1],samplesR[,1])
qqplot(samplescpp[,2],samplesR[,2])
```

从图中可以看出，两种方法生成的随机样本很接近

### 3

Campare the computation time of the two functions with the function “microbenchmark”.

```{r}
library(microbenchmark)
ts<-microbenchmark(gibbs_sampler(n,a,b,N,init_x,init_y),gibbs(N))
summary(ts)
```

从结果可以看出，用C++编写的函数运行时间明显小于R语言编写的函数的运行时间，计算成本更低。




